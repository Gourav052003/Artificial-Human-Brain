{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1fff43c",
   "metadata": {},
   "source": [
    "#  Conversational AI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "553844a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc894367",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/blenderbot-400M-distill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea1cbf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8fbe32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_talk=pipeline('conversational',model=model,tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d801f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Conversation              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07900d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9e4cdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program to translate\n",
    "# speech to text and text to speech\n",
    " \n",
    "import speech_recognition as sr\n",
    "import pyttsx3\n",
    " \n",
    "# Initialize the recognizer\n",
    "r = sr.Recognizer()\n",
    " \n",
    "# Function to convert text to\n",
    "# speech\n",
    "def SpeakText(command):\n",
    "     \n",
    "    # Initialize the engine\n",
    "    engine = pyttsx3.init()\n",
    "    engine.say(command)\n",
    "    engine.runAndWait()\n",
    "     \n",
    "# Loop infinitely for user to\n",
    "# speak\n",
    " \n",
    "while(1):   \n",
    "    print(\"===Speak====\")\n",
    "    # Exception handling to handle\n",
    "    # exceptions at the runtime\n",
    "    try:\n",
    "         \n",
    "        # use the microphone as source for input.\n",
    "        with sr.Microphone() as source2:\n",
    "             \n",
    "            # wait for a second to let the recognizer\n",
    "            # adjust the energy threshold based on\n",
    "            # the surrounding noise level\n",
    "            r.adjust_for_ambient_noise(source2, duration=0.2)\n",
    "             \n",
    "            #listens for the user's input\n",
    "            audio2 = r.listen(source2)\n",
    "             \n",
    "            # Using google to recognize audio\n",
    "            MyText = r.recognize_google(audio2)\n",
    "            MyText = MyText.lower()\n",
    "             \n",
    "            print(\"Me : \"+MyText,end=\"\")\n",
    "            conv1_start = Conversation(MyText)\n",
    "            talk=conv_talk(conv1_start)\n",
    "            talk=str(talk).replace(\" \\n\",\"\").split(\"bot >>  \")[1]\n",
    "            print(\"\\nBot : \"+talk)    \n",
    "            \n",
    "            SpeakText(talk)\n",
    "             \n",
    "    except sr.RequestError as e:\n",
    "        SpeakText(\"Sorry! Can you say it again? \")\n",
    "         \n",
    "    except sr.UnknownValueError:\n",
    "        SpeakText(\"Sorry! Can you say it again? \")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f8f48c",
   "metadata": {},
   "source": [
    "# Instance sementic segemtation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c234e598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "import keras.layers.normalization.batch_normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f6fde27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Main directory\\Computer science\\Natural language processing\\Artificial Human brain\\AHB\\Lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Main directory\\Computer science\\Natural language processing\\Artificial Human brain\\AHB\\Lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n"
     ]
    }
   ],
   "source": [
    "import pixellib\n",
    "from pixellib.instance import instance_segmentation\n",
    "import cv2\n",
    "\n",
    "segment_video = instance_segmentation()\n",
    "segment_video.load_model(\"mask_rcnn_coco.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e507ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Main directory\\Computer science\\Natural language processing\\Artificial Human brain\\AHB\\Lib\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    }
   ],
   "source": [
    "# camera = \"http://192.168.137.6:8080/video\"\n",
    "# capture = cv2.VideoCapture(camera)\n",
    "capture = cv2.VideoCapture(0)\n",
    "# capture.open(camera)\n",
    "segment_video.process_camera(capture,show_bboxes = True, frames_per_second= 15, show_frames= True,\n",
    "frame_name= \"frame\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc818f2",
   "metadata": {},
   "source": [
    "# Distance measuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ecadedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install opencv \"pip install opencv-python\"\n",
    "import cv2\n",
    " \n",
    "# distance from camera to object(face) measured\n",
    "# centimeter\n",
    "Known_distance = 55\n",
    " \n",
    "# width of face in the real world or Object Plane\n",
    "# centimeter\n",
    "Known_width = 14.3\n",
    " \n",
    "# Colors\n",
    "GREEN = (0, 255, 0)\n",
    "RED = (0, 0, 255)\n",
    "WHITE = (255, 255, 255)\n",
    "BLACK = (0, 0, 0)\n",
    " \n",
    "# defining the fonts\n",
    "fonts = cv2.FONT_HERSHEY_COMPLEX\n",
    " \n",
    "# face detector object\n",
    "face_detector = cv2.CascadeClassifier(\"D:\\\\Main directory\\\\Computer science\\\\Natural language processing\\\\Artificial Human brain\\\\AHB\\\\Lib\\\\site-packages\\\\cv2\\\\data\\\\haarcascade_frontalface_default.xml\")\n",
    " \n",
    "# face_detector = cv2.CascadeClassifier(\"mask_rcnn_coco.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eafe3db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# focal length finder function\n",
    "def Focal_Length_Finder(measured_distance, real_width, width_in_rf_image):\n",
    " \n",
    "    # finding the focal length\n",
    "    focal_length = (width_in_rf_image * measured_distance) / real_width\n",
    "    return focal_length\n",
    " \n",
    "# distance estimation function\n",
    "def Distance_finder(Focal_Length, real_face_width, face_width_in_frame):\n",
    " \n",
    "    distance = (real_face_width * Focal_Length)/face_width_in_frame\n",
    " \n",
    "    # return the distance\n",
    "    return distance\n",
    " \n",
    "def face_data(image):\n",
    " \n",
    "    face_width = 0  # making face width to zero\n",
    " \n",
    "    # converting color image to gray scale image\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    " \n",
    "    # detecting face in the image\n",
    "    faces = face_detector.detectMultiScale(gray_image, 1.3, 5)\n",
    " \n",
    "    # looping through the faces detect in the image\n",
    "    # getting coordinates x, y , width and height\n",
    "    for (x, y, h, w) in faces:\n",
    " \n",
    "        # draw the rectangle on the face\n",
    "        cv2.rectangle(image, (x, y), (x+w, y+h), GREEN, 2)\n",
    " \n",
    "        # getting face width in the pixels\n",
    "        face_width = w\n",
    " \n",
    "    # return the face width in pixel\n",
    "    return face_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adea7edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "896.1538461538461\n"
     ]
    }
   ],
   "source": [
    "# reading reference_image from directory\n",
    "ref_image = cv2.imread(\"Ref_image2.jpg\")\n",
    " \n",
    "# find the face width(pixels) in the reference_image\n",
    "ref_image_face_width = face_data(ref_image)\n",
    " \n",
    "# get the focal by calling \"Focal_Length_Finder\"\n",
    "# face width in reference(pixels),\n",
    "# Known_distance(centimeters)\n",
    "# known_width(centimeters)\n",
    "Focal_length_found = Focal_Length_Finder(\n",
    "    Known_distance, Known_width, ref_image_face_width)\n",
    " \n",
    "print(Focal_length_found)\n",
    " \n",
    "# show the reference image\n",
    "cv2.imshow(\"ref_image\", ref_image)\n",
    " \n",
    "# initialize the camera object so that we\n",
    "# can get frame from it\n",
    "cap = cv2.VideoCapture(0)\n",
    "# camera = \"http://192.168.29.128:8080/video\"\n",
    "# cap.open(camera)\n",
    "# looping through frame, incoming from\n",
    "# camera/video\n",
    "while True:\n",
    " \n",
    "    # reading the frame from camera\n",
    "    _, frame = cap.read()\n",
    " \n",
    "    # calling face_data function to find\n",
    "    # the width of face(pixels) in the frame\n",
    "    face_width_in_frame = face_data(frame)\n",
    " \n",
    "    # check if the face is zero then not\n",
    "    # find the distance\n",
    "    if face_width_in_frame != 0:\n",
    "       \n",
    "        # finding the distance by calling function\n",
    "        # Distance finder function need\n",
    "        # these arguments the Focal_Length,\n",
    "        # Known_width(centimeters),\n",
    "        # and Known_distance(centimeters)\n",
    "        Distance = Distance_finder(\n",
    "            Focal_length_found, Known_width, face_width_in_frame)\n",
    " \n",
    "        # draw line as background of text\n",
    "        cv2.line(frame, (30, 30), (230, 30), RED, 32)\n",
    "        cv2.line(frame, (30, 30), (230, 30), BLACK, 28)\n",
    " \n",
    "        # Drawing Text on the screen\n",
    "        cv2.putText(\n",
    "            frame, f\"Distance: {round(Distance,2)} CM\", (30, 35),\n",
    "          fonts, 0.6, GREEN, 2)\n",
    " \n",
    "    # show the frame on the screen\n",
    "    cv2.imshow(\"frame\", frame)\n",
    " \n",
    "    # quit the program if you press 'q' on keyboard\n",
    "    if cv2.waitKey(1) == ord(\"q\"):\n",
    "        break\n",
    "        \n",
    "# closing the camera\n",
    "cap.release()\n",
    " \n",
    "# closing the windows that are opened\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f112f3",
   "metadata": {},
   "source": [
    "# Mutipose detection using movnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cb08655",
   "metadata": {},
   "outputs": [],
   "source": [
    "    import tensorflow as tf\n",
    "    import tensorflow_hub as hub\n",
    "    import cv2\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4045bed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional if you are using a GPU(for setting limit for memory usage)\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# for gpu in gpus:\n",
    "#     tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03cae78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "model = hub.load('https://tfhub.dev/google/movenet/multipose/lightning/1')\n",
    "movnet = model.signatures['serving_default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b93bc3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to loop through each person detected and render\n",
    "def loop_through_people(frame, keypoints_with_scores, edges, confidence_threshold):\n",
    "    for person in keypoints_with_scores:\n",
    "        draw_connections(frame, person, edges, confidence_threshold)\n",
    "        draw_keypoints(frame, person, confidence_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d69f957",
   "metadata": {},
   "outputs": [],
   "source": [
    "EDGES = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ef6ae3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw keypoints\n",
    "\n",
    "def draw_keypoints(frame, keypoints, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
    "    \n",
    "    for kp in shaped:\n",
    "        ky, kx, kp_conf = kp\n",
    "        if kp_conf > confidence_threshold:\n",
    "            cv2.circle(frame, (int(kx), int(ky)), 6, (0,255,0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f9b738b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw connections\n",
    "\n",
    "def draw_connections(frame, keypoints, edges, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
    "    \n",
    "    for edge, color in edges.items():\n",
    "        p1, p2 = edge\n",
    "        y1, x1, c1 = shaped[p1]\n",
    "        y2, x2, c2 = shaped[p2]\n",
    "        \n",
    "        if (c1 > confidence_threshold) & (c2 > confidence_threshold):      \n",
    "            cv2.line(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0,0,255), 4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2cba308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make detections\n",
    "cap = cv2.VideoCapture(0)\n",
    "# camera = \"http://192.168.29.128:8080/video\"\n",
    "# cap.open(camera)\n",
    "\n",
    "\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    \n",
    "    ret,frame = cap.read()\n",
    "    \n",
    "    #resize image\n",
    "    \n",
    "    img = frame.copy()\n",
    "    img = tf.image.resize_with_pad(tf.expand_dims(img, axis=0),384,640)\n",
    "    input_img = tf.cast(img, dtype = tf.int32)\n",
    "    \n",
    "    # Detection section\n",
    "    results = movnet(input_img)\n",
    "    keypoints_with_scores = results['output_0'].numpy()[:,:,:51].reshape((6,17,3))\n",
    "    \n",
    "    # Render keypoints\n",
    "    loop_through_people(frame, keypoints_with_scores, EDGES, 0.3)\n",
    "    \n",
    "    cv2.imshow(\"Movnet Multipose\",frame)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF ==ord('q'):\n",
    "#         print(frame.shape)\n",
    "#         print(frame)\n",
    "#         print(tf.expand_dims(frame,axis=0))\n",
    "#         print(tf.image.resize_with_pad(tf.expand_dims(frame,axis=0),384,640))\n",
    "#         print(tf.cast(tf.image.resize_with_pad(tf.expand_dims(frame,axis=0),384,640)))\n",
    "        \n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AHB",
   "language": "python",
   "name": "ahb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
